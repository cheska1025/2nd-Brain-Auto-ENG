// optimize-ai-models.js - AI ëª¨ë¸ ìµœì í™”
const axios = require('axios');
const Redis = require('ioredis');

class AIOptimizer {
  constructor() {
    this.localAIUrl = 'http://localhost:11434';
    this.redis = new Redis({
      host: 'localhost',
      port: 6379,
      password: 'redis_password'
    });
  }

  async optimizeAll() {
    console.log('âš¡ AI ëª¨ë¸ ìµœì í™” ì‹œì‘...');
    
    const results = await Promise.allSettled([
      this.optimizeLocalModels(),
      this.clearAICache(),
      this.updateModelWeights(),
      this.optimizeMemoryUsage()
    ]);

    this.reportOptimizationResults(results);
    console.log('âœ… AI ìµœì í™” ì™„ë£Œ!');
  }

  async optimizeLocalModels() {
    try {
      console.log('ğŸ”„ ë¡œì»¬ AI ëª¨ë¸ ìµœì í™” ì¤‘...');
      
      const response = await axios.get(`${this.localAIUrl}/api/tags`);
      const models = response.data.models || [];
      
      if (models.length === 0) {
        console.log('âš ï¸ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.');
        return { status: 'warning', message: 'ëª¨ë¸ ì—†ìŒ' };
      }

      const optimizationResults = [];
      
      for (const model of models) {
        console.log(`  ğŸ”§ ${model.name} ìµœì í™” ì¤‘...`);
        
        try {
          // ëª¨ë¸ ì‚¬ì „ ë¡œë“œ (ì²« ì‹¤í–‰ ì†ë„ ê°œì„ )
          await axios.post(`${this.localAIUrl}/api/generate`, {
            model: model.name,
            prompt: 'hello',
            stream: false,
            options: {
              temperature: 0.1,
              top_p: 0.1,
              max_tokens: 10
            }
          });
          
          optimizationResults.push({
            model: model.name,
            status: 'optimized',
            size: model.size
          });
          
        } catch (error) {
          console.warn(`    âš ï¸ ${model.name} ìµœì í™” ì‹¤íŒ¨: ${error.message}`);
          optimizationResults.push({
            model: model.name,
            status: 'failed',
            error: error.message
          });
        }
      }
      
      return {
        status: 'success',
        models: optimizationResults,
        totalModels: models.length
      };
      
    } catch (error) {
      console.error('ë¡œì»¬ ëª¨ë¸ ìµœì í™” ì‹¤íŒ¨:', error);
      return { status: 'failed', error: error.message };
    }
  }

  async clearAICache() {
    try {
      console.log('ğŸ—‘ï¸ AI ìºì‹œ ì •ë¦¬ ì¤‘...');
      
      // AI ê´€ë ¨ ìºì‹œ í‚¤ë“¤ë§Œ ì‚­ì œ
      const keys = await this.redis.keys('ai:*');
      if (keys.length > 0) {
        await this.redis.del(...keys);
        console.log(`  âœ… AI ìºì‹œ ${keys.length}ê°œ í•­ëª© ì •ë¦¬ ì™„ë£Œ`);
        return { status: 'success', clearedKeys: keys.length };
      } else {
        console.log('  â„¹ï¸ ì •ë¦¬í•  AI ìºì‹œê°€ ì—†ìŠµë‹ˆë‹¤.');
        return { status: 'skipped', message: 'ìºì‹œ ì—†ìŒ' };
      }
    } catch (error) {
      console.error('AI ìºì‹œ ì •ë¦¬ ì‹¤íŒ¨:', error);
      return { status: 'failed', error: error.message };
    }
  }

  async updateModelWeights() {
    try {
      console.log('âš–ï¸ ëª¨ë¸ ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸ ì¤‘...');
      
      // ëª¨ë¸ ì„±ëŠ¥ ë°ì´í„° ìˆ˜ì§‘
      const performanceData = await this.collectPerformanceData();
      
      // ê°€ì¤‘ì¹˜ ê³„ì‚°
      const weights = this.calculateModelWeights(performanceData);
      
      // Redisì— ê°€ì¤‘ì¹˜ ì €ì¥
      await this.redis.hset('ai:model_weights', weights);
      
      console.log('  âœ… ëª¨ë¸ ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸ ì™„ë£Œ');
      return { status: 'success', weights: weights };
      
    } catch (error) {
      console.error('ëª¨ë¸ ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨:', error);
      return { status: 'failed', error: error.message };
    }
  }

  async collectPerformanceData() {
    // ì‹¤ì œë¡œëŠ” ì„±ëŠ¥ ë©”íŠ¸ë¦­ì„ ìˆ˜ì§‘í•˜ì§€ë§Œ, ì—¬ê¸°ì„œëŠ” ì‹œë®¬ë ˆì´ì…˜
    const models = ['llama2', 'codellama'];
    const performanceData = {};
    
    for (const model of models) {
      try {
        const startTime = Date.now();
        await axios.post(`${this.localAIUrl}/api/generate`, {
          model: model,
          prompt: 'test',
          stream: false,
          options: { max_tokens: 10 }
        });
        const responseTime = Date.now() - startTime;
        
        performanceData[model] = {
          responseTime: responseTime,
          successRate: 1.0,
          lastUsed: new Date().toISOString()
        };
      } catch (error) {
        performanceData[model] = {
          responseTime: 9999,
          successRate: 0.0,
          lastUsed: new Date().toISOString()
        };
      }
    }
    
    return performanceData;
  }

  calculateModelWeights(performanceData) {
    const weights = {};
    const totalModels = Object.keys(performanceData).length;
    
    Object.entries(performanceData).forEach(([model, data]) => {
      // ì‘ë‹µ ì‹œê°„ì´ ë¹ ë¥´ê³  ì„±ê³µë¥ ì´ ë†’ì„ìˆ˜ë¡ ë†’ì€ ê°€ì¤‘ì¹˜
      const timeScore = Math.max(0, 1 - (data.responseTime / 10000));
      const successScore = data.successRate;
      const weight = (timeScore + successScore) / 2;
      
      weights[model] = weight.toFixed(3);
    });
    
    return weights;
  }

  async optimizeMemoryUsage() {
    try {
      console.log('ğŸ§  ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì í™” ì¤‘...');
      
      // Redis ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
      const memoryInfo = await this.redis.memory('usage');
      const maxMemory = await this.redis.config('get', 'maxmemory');
      
      console.log(`  ğŸ“Š í˜„ì¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: ${(memoryInfo / 1024 / 1024).toFixed(2)}MB`);
      
      // ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ 80% ì´ìƒì´ë©´ ì •ë¦¬
      const memoryUsagePercent = (memoryInfo / parseInt(maxMemory[1])) * 100;
      
      if (memoryUsagePercent > 80) {
        console.log('  ğŸ§¹ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ë†’ì•„ ìºì‹œ ì •ë¦¬ ì¤‘...');
        
        // ì˜¤ë˜ëœ ìºì‹œ í‚¤ ì‚­ì œ
        const oldKeys = await this.redis.keys('*');
        const keysToDelete = oldKeys.slice(0, Math.floor(oldKeys.length * 0.2)); // 20% ì‚­ì œ
        
        if (keysToDelete.length > 0) {
          await this.redis.del(...keysToDelete);
          console.log(`  âœ… ${keysToDelete.length}ê°œ í‚¤ ì‚­ì œ ì™„ë£Œ`);
        }
        
        return { status: 'success', action: 'cache_cleared', keysDeleted: keysToDelete.length };
      } else {
        console.log('  â„¹ï¸ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ì •ìƒ ë²”ìœ„ì…ë‹ˆë‹¤.');
        return { status: 'skipped', message: 'ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì •ìƒ' };
      }
      
    } catch (error) {
      console.error('ë©”ëª¨ë¦¬ ìµœì í™” ì‹¤íŒ¨:', error);
      return { status: 'failed', error: error.message };
    }
  }

  reportOptimizationResults(results) {
    console.log('\nğŸ“Š ìµœì í™” ê²°ê³¼ ìš”ì•½:');
    
    results.forEach((result, index) => {
      const taskNames = ['ë¡œì»¬ ëª¨ë¸', 'ìºì‹œ ì •ë¦¬', 'ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸', 'ë©”ëª¨ë¦¬ ìµœì í™”'];
      const taskName = taskNames[index] || `ì‘ì—… ${index + 1}`;
      
      if (result.status === 'fulfilled') {
        const data = result.value;
        const statusIcon = data.status === 'success' ? 'âœ…' : 
                          data.status === 'skipped' ? 'â­ï¸' : 'âŒ';
        
        console.log(`  ${statusIcon} ${taskName}: ${data.status}`);
        
        if (data.models) {
          console.log(`     ìµœì í™”ëœ ëª¨ë¸: ${data.models.length}ê°œ`);
        }
        if (data.clearedKeys) {
          console.log(`     ì •ë¦¬ëœ ìºì‹œ: ${data.clearedKeys}ê°œ`);
        }
        if (data.keysDeleted) {
          console.log(`     ì‚­ì œëœ í‚¤: ${data.keysDeleted}ê°œ`);
        }
        if (data.error) {
          console.log(`     ì˜¤ë¥˜: ${data.error}`);
        }
      } else {
        console.log(`  âŒ ${taskName}: ì‹¤íŒ¨ - ${result.reason}`);
      }
    });
  }

  async getModelStatus() {
    try {
      const response = await axios.get(`${this.localAIUrl}/api/tags`);
      const models = response.data.models || [];
      
      console.log('ğŸ¤– ë¡œì»¬ AI ëª¨ë¸ ìƒíƒœ:');
      models.forEach(model => {
        const size = (model.size / 1024 / 1024 / 1024).toFixed(2);
        console.log(`  ğŸ“¦ ${model.name} (${size}GB)`);
      });
      
      return models;
    } catch (error) {
      console.error('ëª¨ë¸ ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨:', error);
      return [];
    }
  }

  async pullModel(modelName) {
    try {
      console.log(`ğŸ“¥ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì¤‘: ${modelName}`);
      
      const response = await axios.post(`${this.localAIUrl}/api/pull`, {
        name: modelName,
        stream: false
      }, {
        timeout: 300000 // 5ë¶„ íƒ€ì„ì•„ì›ƒ
      });
      
      console.log(`âœ… ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: ${modelName}`);
      return { status: 'success', model: modelName };
      
    } catch (error) {
      console.error(`ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: ${modelName}`, error);
      return { status: 'failed', model: modelName, error: error.message };
    }
  }

  async disconnect() {
    if (this.redis) {
      await this.redis.disconnect();
    }
  }
}

// CLI ì‹¤í–‰ ë¶€ë¶„
if (require.main === module) {
  const command = process.argv[2];
  const optimizer = new AIOptimizer();
  
  switch (command) {
    case 'optimize':
      optimizer.optimizeAll()
        .then(() => optimizer.disconnect())
        .catch(error => {
          console.error('ìµœì í™” ì‹¤íŒ¨:', error);
          optimizer.disconnect();
          process.exit(1);
        });
      break;
      
    case 'status':
      optimizer.getModelStatus()
        .then(() => optimizer.disconnect())
        .catch(error => {
          console.error('ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨:', error);
          optimizer.disconnect();
          process.exit(1);
        });
      break;
      
    case 'pull':
      const modelName = process.argv[3];
      if (!modelName) {
        console.log('ì‚¬ìš©ë²•: node scripts/optimize-ai-models.js pull <model-name>');
        process.exit(1);
      }
      
      optimizer.pullModel(modelName)
        .then(() => optimizer.disconnect())
        .catch(error => {
          console.error('ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨:', error);
          optimizer.disconnect();
          process.exit(1);
        });
      break;
      
    default:
      console.log(`
ì‚¬ìš©ë²•: node scripts/optimize-ai-models.js <command>

ëª…ë ¹ì–´:
  optimize  - AI ëª¨ë¸ ìµœì í™” ì‹¤í–‰
  status    - ëª¨ë¸ ìƒíƒœ ì¡°íšŒ
  pull      - ëª¨ë¸ ë‹¤ìš´ë¡œë“œ

ì˜ˆì œ:
  node scripts/optimize-ai-models.js optimize
  node scripts/optimize-ai-models.js status
  node scripts/optimize-ai-models.js pull llama2
      `);
  }
}

module.exports = AIOptimizer;
